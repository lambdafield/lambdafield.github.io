데이터-흐름과-관리
01/24/2014, 06:42:20
data
mongodb,pandas,hdf5,h5py,scikit-learn
데이터를 처리하는 프로젝트를 하지 않고 이런 저런 생각을 해 보니, 검증되지 않은 여러가지 생각을 하게 된다.
보통 데이터는 데이터베이스에 저장된다. 생성, 갱신, 삭제와 같은 기본적인 기능을 충실히 사용할 수 있을 뿐만 아니라, 백업등 다양한 기능을 지원하기 때문이다.
하지만 기계 학습에 바로 적용하기에는 부족하다. 우선, 정제되지 않은 데이터일 경우가 많을 수 있고 
기계 학습에서는 전처리 과정이 매우 중요하여 데이터를 계속적으로 변형해야 할 경우가 많은데 그때마다 계속 저장하는게 깔끔하지 않다.
그래서 다음과 같은 데이터의 흐름을 생각해 본다.

Mongodb -> Pandas 

[mongodb 데이터를 pandas 데이터로 변환하기](5657382461898752)에서 mongodb 데이터를 DataFrame으로 변환한다.
이렇게 함으로써 이후부터는 얼마든지 자유롭게 데이터를 가공할 수 있다. 인코딩을 하거나 표준화나 정규화를 하거나 일부 필요없는 값들은 지울 수도 있다.
이렇게 처리된 데이터는 다음 번에 사용할 수 있도록 다시 저장하면 좋다. 
데이터가 크면 클수록 처리하는 시간이 길어지니 이후 시간 절약을 위해 필요한 작업이라 할 수 있다.
  
Mongodb -> Pandas -> HDF5

이 단계에서 HDF5를 사용하여 저장한다([HDF5로 데이터 저장하기](5733935958982656)).
기계 학습에서는 수치형으로 데이터를 처리하기 때문에 HDF5에 numpy의 array로 저장하는 장점 잘 살릴 수 있다.

여기까지는 단계적으로 처리 할 수 있고, 해 보았지만 더 나아가 데이터간에 관계나 최종 관계를 DB에 다시 저장하면 시스템에서 사용하기 편할 듯 하다.

Mongodb -> Pandas -> HDF5 -> Mongodb

좀 더 생각해 보아야겠지만 뭐 어차피 프로젝트에 사용해 보거나 사용할지 모르는 상태에서 잡생각같다.